system:
  embedding_model: all-MiniLM-L6-v2
  llm:
    provider: huggingface
    model_path: meta-llama/Llama-3.1-8B-Instruct
    device: auto
    temperature: 0.0
  judge_llm: llama3
paths:
  chroma_db: data/chroma_db
  results: data/results
  cache: data/raw
data:
  ingestion_size: 700
  ingestion_seed: 42
  test_size: 1
  test_seed: 123
evaluation:
  deepeval_max_concurrent: 5
retrieval:
  top_k: 5
  chunk_size: 512
  chunk_overlap: 50
defenses:
- name: differential_privacy
  enabled: true
  method: dp_approx
  epsilon: 3.0
  delta: 0.01
  candidate_multiplier: 3
- name: trustrag
  enabled: false
  similarity_threshold: 0.88
  rouge_threshold: 0.25
  candidate_multiplier: 3
- name: attention_filtering
  enabled: false
  model_path: meta-llama/Llama-3.1-8B-Instruct
  top_tokens: 100
  max_corruptions: 3
  short_answer_threshold: 50
  device: cuda
  candidate_multiplier: 3
attack:
  mba:
    M: 7  # Number of masks per chunk (optimal: 3-5 for 512-char chunks)
    gamma: 0.5  # Membership threshold (accuracy > gamma = member)
    num_members: 50  # Number of member chunks to test (QUICK TEST - use 50+ for real experiments)
    num_non_members: 50  # Number of non-member chunks to test (QUICK TEST - use 50+ for real experiments)
    device: auto  # cuda, cpu, or auto
    proxy_model: gpt2-xl  # gpt2 (small/fast), gpt2-medium, gpt2-large, gpt2-xl (slow)
    enable_spelling_correction: false  # Disable spelling correction (saves memory)
    max_document_words: 200  # Truncate documents longer than this (GPT-2 context limit, reduced for chunks)
ado:
  enabled: true
  user_id: "test_user_001"
  sentinel_model: "llama3" # local ollama model
  strategist_model: "llama3"
  trust_score_decay: 0.05
